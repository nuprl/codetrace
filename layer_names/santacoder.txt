## model._no_split_modules
['GPT2Block']

## model.__dict__

{'training': False,
 '_parameters': OrderedDict(),
 '_buffers': OrderedDict(),
 '_non_persistent_buffers_set': set(),
 '_backward_hooks': OrderedDict(),
 '_is_full_backward_hook': None,
 '_forward_hooks': OrderedDict(),
 '_forward_pre_hooks': OrderedDict(),
 '_state_dict_hooks': OrderedDict(),
 '_load_state_dict_pre_hooks': OrderedDict(),
 '_load_state_dict_post_hooks': OrderedDict(),
 '_modules': OrderedDict([('transformer',
               GPT2CustomModel(
                 (wte): Embedding(49280, 2048)
                 (wpe): Embedding(2048, 2048)
                 (drop): Dropout(p=0.1, inplace=False)
                 (h): ModuleList(
                   (0): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (1): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (2): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (3): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (4): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (5): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (6): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (7): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (8): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (9): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (10): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (11): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (12): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (13): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (14): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (15): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (16): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (17): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (18): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (19): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (20): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (21): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (22): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                   (23): GPT2CustomBlock(
                     (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (attn): GPT2MQAttention(
                       (q_attn): Conv1D()
                       (kv_attn): Conv1D()
                       (c_proj): Conv1D()
                       (attn_dropout): Dropout(p=0.1, inplace=False)
                       (resid_dropout): Dropout(p=0.1, inplace=False)
                     )
                     (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
                     (mlp): GPT2MLP(
                       (c_fc): Conv1D()
                       (c_proj): Conv1D()
                       (act): FastGELUActivation()
                       (dropout): Dropout(p=0.1, inplace=False)
                     )
                   )
                 )
                 (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
               )),
              ('lm_head',
               Linear(in_features=2048, out_features=49280, bias=False))]),
 'config': GPT2CustomConfig {
   "_name_or_path": "bigcode/santacoder",
   "activation_function": "gelu_fast",
   "architectures": [
     "GPT2LMHeadCustomModel"
   ],
   "attention_head_type": "multiquery",
   "attn_pdrop": 0.1,
   "auto_map": {
     "AutoConfig": "configuration_gpt2_mq.GPT2CustomConfig",
     "AutoModelForCausalLM": "modeling_gpt2_mq.GPT2LMHeadCustomModel"
   },
   "bos_token_id": 49152,
   "embd_pdrop": 0.1,
   "eos_token_id": 49152,
   "initializer_range": 0.02,
   "layer_norm_epsilon": 1e-05,
   "model_type": "gpt2",
   "n_embd": 2048,
   "n_head": 16,
   "n_inner": 8192,
   "n_layer": 24,
   "n_positions": 2048,
   "reorder_and_upcast_attn": false,
   "resid_pdrop": 0.1,
   "scale_attn_by_inverse_layer_idx": false,
   "scale_attn_weights": true,
   "summary_activation": null,
   "summary_first_dropout": 0.1,
   "summary_proj_to_labels": true,
   "summary_type": "cls_index",
   "summary_use_proj": true,
   "torch_dtype": "float16",
   "transformers_version": "4.25.1",
   "use_cache": true,
   "vocab_size": 49280
 },
 'name_or_path': 'bigcode/santacoder',
 'warnings_issued': {},
 'model_parallel': False,
 'device_map': None,
 'is_loaded_in_8bit': False}
 
 
 ## for i in model.named_parameters(): print(i[0])
 
transformer.wte.weight
transformer.wpe.weight
transformer.h.0.ln_1.weight
transformer.h.0.ln_1.bias
transformer.h.0.attn.q_attn.weight
transformer.h.0.attn.q_attn.bias
transformer.h.0.attn.kv_attn.weight
transformer.h.0.attn.kv_attn.bias
transformer.h.0.attn.c_proj.weight
transformer.h.0.attn.c_proj.bias
transformer.h.0.ln_2.weight
transformer.h.0.ln_2.bias
transformer.h.0.mlp.c_fc.weight
transformer.h.0.mlp.c_fc.bias
transformer.h.0.mlp.c_proj.weight
transformer.h.0.mlp.c_proj.bias
transformer.h.1.ln_1.weight
transformer.h.1.ln_1.bias
transformer.h.1.attn.q_attn.weight
transformer.h.1.attn.q_attn.bias
transformer.h.1.attn.kv_attn.weight
transformer.h.1.attn.kv_attn.bias
transformer.h.1.attn.c_proj.weight
transformer.h.1.attn.c_proj.bias
transformer.h.1.ln_2.weight
transformer.h.1.ln_2.bias
transformer.h.1.mlp.c_fc.weight
transformer.h.1.mlp.c_fc.bias
transformer.h.1.mlp.c_proj.weight
transformer.h.1.mlp.c_proj.bias
transformer.h.2.ln_1.weight
transformer.h.2.ln_1.bias
transformer.h.2.attn.q_attn.weight
transformer.h.2.attn.q_attn.bias
transformer.h.2.attn.kv_attn.weight
transformer.h.2.attn.kv_attn.bias
transformer.h.2.attn.c_proj.weight
transformer.h.2.attn.c_proj.bias
transformer.h.2.ln_2.weight
transformer.h.2.ln_2.bias
transformer.h.2.mlp.c_fc.weight
transformer.h.2.mlp.c_fc.bias
transformer.h.2.mlp.c_proj.weight
transformer.h.2.mlp.c_proj.bias
transformer.h.3.ln_1.weight
transformer.h.3.ln_1.bias
transformer.h.3.attn.q_attn.weight
transformer.h.3.attn.q_attn.bias
transformer.h.3.attn.kv_attn.weight
transformer.h.3.attn.kv_attn.bias
transformer.h.3.attn.c_proj.weight
transformer.h.3.attn.c_proj.bias
transformer.h.3.ln_2.weight
transformer.h.3.ln_2.bias
transformer.h.3.mlp.c_fc.weight
transformer.h.3.mlp.c_fc.bias
transformer.h.3.mlp.c_proj.weight
transformer.h.3.mlp.c_proj.bias
transformer.h.4.ln_1.weight
transformer.h.4.ln_1.bias
transformer.h.4.attn.q_attn.weight
transformer.h.4.attn.q_attn.bias
transformer.h.4.attn.kv_attn.weight
transformer.h.4.attn.kv_attn.bias
transformer.h.4.attn.c_proj.weight
transformer.h.4.attn.c_proj.bias
transformer.h.4.ln_2.weight
transformer.h.4.ln_2.bias
transformer.h.4.mlp.c_fc.weight
transformer.h.4.mlp.c_fc.bias
transformer.h.4.mlp.c_proj.weight
transformer.h.4.mlp.c_proj.bias
transformer.h.5.ln_1.weight
transformer.h.5.ln_1.bias
transformer.h.5.attn.q_attn.weight
transformer.h.5.attn.q_attn.bias
transformer.h.5.attn.kv_attn.weight
transformer.h.5.attn.kv_attn.bias
transformer.h.5.attn.c_proj.weight
transformer.h.5.attn.c_proj.bias
transformer.h.5.ln_2.weight
transformer.h.5.ln_2.bias
transformer.h.5.mlp.c_fc.weight
transformer.h.5.mlp.c_fc.bias
transformer.h.5.mlp.c_proj.weight
transformer.h.5.mlp.c_proj.bias
transformer.h.6.ln_1.weight
transformer.h.6.ln_1.bias
transformer.h.6.attn.q_attn.weight
transformer.h.6.attn.q_attn.bias
transformer.h.6.attn.kv_attn.weight
transformer.h.6.attn.kv_attn.bias
transformer.h.6.attn.c_proj.weight
transformer.h.6.attn.c_proj.bias
transformer.h.6.ln_2.weight
transformer.h.6.ln_2.bias
transformer.h.6.mlp.c_fc.weight
transformer.h.6.mlp.c_fc.bias
transformer.h.6.mlp.c_proj.weight
transformer.h.6.mlp.c_proj.bias
transformer.h.7.ln_1.weight
transformer.h.7.ln_1.bias
transformer.h.7.attn.q_attn.weight
transformer.h.7.attn.q_attn.bias
transformer.h.7.attn.kv_attn.weight
transformer.h.7.attn.kv_attn.bias
transformer.h.7.attn.c_proj.weight
transformer.h.7.attn.c_proj.bias
transformer.h.7.ln_2.weight
transformer.h.7.ln_2.bias
transformer.h.7.mlp.c_fc.weight
transformer.h.7.mlp.c_fc.bias
transformer.h.7.mlp.c_proj.weight
transformer.h.7.mlp.c_proj.bias
transformer.h.8.ln_1.weight
transformer.h.8.ln_1.bias
transformer.h.8.attn.q_attn.weight
transformer.h.8.attn.q_attn.bias
transformer.h.8.attn.kv_attn.weight
transformer.h.8.attn.kv_attn.bias
transformer.h.8.attn.c_proj.weight
transformer.h.8.attn.c_proj.bias
transformer.h.8.ln_2.weight
transformer.h.8.ln_2.bias
transformer.h.8.mlp.c_fc.weight
transformer.h.8.mlp.c_fc.bias
transformer.h.8.mlp.c_proj.weight
transformer.h.8.mlp.c_proj.bias
transformer.h.9.ln_1.weight
transformer.h.9.ln_1.bias
transformer.h.9.attn.q_attn.weight
transformer.h.9.attn.q_attn.bias
transformer.h.9.attn.kv_attn.weight
transformer.h.9.attn.kv_attn.bias
transformer.h.9.attn.c_proj.weight
transformer.h.9.attn.c_proj.bias
transformer.h.9.ln_2.weight
transformer.h.9.ln_2.bias
transformer.h.9.mlp.c_fc.weight
transformer.h.9.mlp.c_fc.bias
transformer.h.9.mlp.c_proj.weight
transformer.h.9.mlp.c_proj.bias
transformer.h.10.ln_1.weight
transformer.h.10.ln_1.bias
transformer.h.10.attn.q_attn.weight
transformer.h.10.attn.q_attn.bias
transformer.h.10.attn.kv_attn.weight
transformer.h.10.attn.kv_attn.bias
transformer.h.10.attn.c_proj.weight
transformer.h.10.attn.c_proj.bias
transformer.h.10.ln_2.weight
transformer.h.10.ln_2.bias
transformer.h.10.mlp.c_fc.weight
transformer.h.10.mlp.c_fc.bias
transformer.h.10.mlp.c_proj.weight
transformer.h.10.mlp.c_proj.bias
transformer.h.11.ln_1.weight
transformer.h.11.ln_1.bias
transformer.h.11.attn.q_attn.weight
transformer.h.11.attn.q_attn.bias
transformer.h.11.attn.kv_attn.weight
transformer.h.11.attn.kv_attn.bias
transformer.h.11.attn.c_proj.weight
transformer.h.11.attn.c_proj.bias
transformer.h.11.ln_2.weight
transformer.h.11.ln_2.bias
transformer.h.11.mlp.c_fc.weight
transformer.h.11.mlp.c_fc.bias
transformer.h.11.mlp.c_proj.weight
transformer.h.11.mlp.c_proj.bias
transformer.h.12.ln_1.weight
transformer.h.12.ln_1.bias
transformer.h.12.attn.q_attn.weight
transformer.h.12.attn.q_attn.bias
transformer.h.12.attn.kv_attn.weight
transformer.h.12.attn.kv_attn.bias
transformer.h.12.attn.c_proj.weight
transformer.h.12.attn.c_proj.bias
transformer.h.12.ln_2.weight
transformer.h.12.ln_2.bias
transformer.h.12.mlp.c_fc.weight
transformer.h.12.mlp.c_fc.bias
transformer.h.12.mlp.c_proj.weight
transformer.h.12.mlp.c_proj.bias
transformer.h.13.ln_1.weight
transformer.h.13.ln_1.bias
transformer.h.13.attn.q_attn.weight
transformer.h.13.attn.q_attn.bias
transformer.h.13.attn.kv_attn.weight
transformer.h.13.attn.kv_attn.bias
transformer.h.13.attn.c_proj.weight
transformer.h.13.attn.c_proj.bias
transformer.h.13.ln_2.weight
transformer.h.13.ln_2.bias
transformer.h.13.mlp.c_fc.weight
transformer.h.13.mlp.c_fc.bias
transformer.h.13.mlp.c_proj.weight
transformer.h.13.mlp.c_proj.bias
transformer.h.14.ln_1.weight
transformer.h.14.ln_1.bias
transformer.h.14.attn.q_attn.weight
transformer.h.14.attn.q_attn.bias
transformer.h.14.attn.kv_attn.weight
transformer.h.14.attn.kv_attn.bias
transformer.h.14.attn.c_proj.weight
transformer.h.14.attn.c_proj.bias
transformer.h.14.ln_2.weight
transformer.h.14.ln_2.bias
transformer.h.14.mlp.c_fc.weight
transformer.h.14.mlp.c_fc.bias
transformer.h.14.mlp.c_proj.weight
transformer.h.14.mlp.c_proj.bias
transformer.h.15.ln_1.weight
transformer.h.15.ln_1.bias
transformer.h.15.attn.q_attn.weight
transformer.h.15.attn.q_attn.bias
transformer.h.15.attn.kv_attn.weight
transformer.h.15.attn.kv_attn.bias
transformer.h.15.attn.c_proj.weight
transformer.h.15.attn.c_proj.bias
transformer.h.15.ln_2.weight
transformer.h.15.ln_2.bias
transformer.h.15.mlp.c_fc.weight
transformer.h.15.mlp.c_fc.bias
transformer.h.15.mlp.c_proj.weight
transformer.h.15.mlp.c_proj.bias
transformer.h.16.ln_1.weight
transformer.h.16.ln_1.bias
transformer.h.16.attn.q_attn.weight
transformer.h.16.attn.q_attn.bias
transformer.h.16.attn.kv_attn.weight
transformer.h.16.attn.kv_attn.bias
transformer.h.16.attn.c_proj.weight
transformer.h.16.attn.c_proj.bias
transformer.h.16.ln_2.weight
transformer.h.16.ln_2.bias
transformer.h.16.mlp.c_fc.weight
transformer.h.16.mlp.c_fc.bias
transformer.h.16.mlp.c_proj.weight
transformer.h.16.mlp.c_proj.bias
transformer.h.17.ln_1.weight
transformer.h.17.ln_1.bias
transformer.h.17.attn.q_attn.weight
transformer.h.17.attn.q_attn.bias
transformer.h.17.attn.kv_attn.weight
transformer.h.17.attn.kv_attn.bias
transformer.h.17.attn.c_proj.weight
transformer.h.17.attn.c_proj.bias
transformer.h.17.ln_2.weight
transformer.h.17.ln_2.bias
transformer.h.17.mlp.c_fc.weight
transformer.h.17.mlp.c_fc.bias
transformer.h.17.mlp.c_proj.weight
transformer.h.17.mlp.c_proj.bias
transformer.h.18.ln_1.weight
transformer.h.18.ln_1.bias
transformer.h.18.attn.q_attn.weight
transformer.h.18.attn.q_attn.bias
transformer.h.18.attn.kv_attn.weight
transformer.h.18.attn.kv_attn.bias
transformer.h.18.attn.c_proj.weight
transformer.h.18.attn.c_proj.bias
transformer.h.18.ln_2.weight
transformer.h.18.ln_2.bias
transformer.h.18.mlp.c_fc.weight
transformer.h.18.mlp.c_fc.bias
transformer.h.18.mlp.c_proj.weight
transformer.h.18.mlp.c_proj.bias
transformer.h.19.ln_1.weight
transformer.h.19.ln_1.bias
transformer.h.19.attn.q_attn.weight
transformer.h.19.attn.q_attn.bias
transformer.h.19.attn.kv_attn.weight
transformer.h.19.attn.kv_attn.bias
transformer.h.19.attn.c_proj.weight
transformer.h.19.attn.c_proj.bias
transformer.h.19.ln_2.weight
transformer.h.19.ln_2.bias
transformer.h.19.mlp.c_fc.weight
transformer.h.19.mlp.c_fc.bias
transformer.h.19.mlp.c_proj.weight
transformer.h.19.mlp.c_proj.bias
transformer.h.20.ln_1.weight
transformer.h.20.ln_1.bias
transformer.h.20.attn.q_attn.weight
transformer.h.20.attn.q_attn.bias
transformer.h.20.attn.kv_attn.weight
transformer.h.20.attn.kv_attn.bias
transformer.h.20.attn.c_proj.weight
transformer.h.20.attn.c_proj.bias
transformer.h.20.ln_2.weight
transformer.h.20.ln_2.bias
transformer.h.20.mlp.c_fc.weight
transformer.h.20.mlp.c_fc.bias
transformer.h.20.mlp.c_proj.weight
transformer.h.20.mlp.c_proj.bias
transformer.h.21.ln_1.weight
transformer.h.21.ln_1.bias
transformer.h.21.attn.q_attn.weight
transformer.h.21.attn.q_attn.bias
transformer.h.21.attn.kv_attn.weight
transformer.h.21.attn.kv_attn.bias
transformer.h.21.attn.c_proj.weight
transformer.h.21.attn.c_proj.bias
transformer.h.21.ln_2.weight
transformer.h.21.ln_2.bias
transformer.h.21.mlp.c_fc.weight
transformer.h.21.mlp.c_fc.bias
transformer.h.21.mlp.c_proj.weight
transformer.h.21.mlp.c_proj.bias
transformer.h.22.ln_1.weight
transformer.h.22.ln_1.bias
transformer.h.22.attn.q_attn.weight
transformer.h.22.attn.q_attn.bias
transformer.h.22.attn.kv_attn.weight
transformer.h.22.attn.kv_attn.bias
transformer.h.22.attn.c_proj.weight
transformer.h.22.attn.c_proj.bias
transformer.h.22.ln_2.weight
transformer.h.22.ln_2.bias
transformer.h.22.mlp.c_fc.weight
transformer.h.22.mlp.c_fc.bias
transformer.h.22.mlp.c_proj.weight
transformer.h.22.mlp.c_proj.bias
transformer.h.23.ln_1.weight
transformer.h.23.ln_1.bias
transformer.h.23.attn.q_attn.weight
transformer.h.23.attn.q_attn.bias
transformer.h.23.attn.kv_attn.weight
transformer.h.23.attn.kv_attn.bias
transformer.h.23.attn.c_proj.weight
transformer.h.23.attn.c_proj.bias
transformer.h.23.ln_2.weight
transformer.h.23.ln_2.bias
transformer.h.23.mlp.c_fc.weight
transformer.h.23.mlp.c_fc.bias
transformer.h.23.mlp.c_proj.weight
transformer.h.23.mlp.c_proj.bias
transformer.ln_f.weight
transformer.ln_f.bias