{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax-dependent trace\n",
    "\n",
    "Head attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "\n",
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (model_utils.py, line 41)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/mechinterp/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 11\u001b[0;36m\n\u001b[0;31m    from model_utils import *\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/codetrace/notebooks/../model_utils.py:41\u001b[0;36m\u001b[0m\n\u001b[0;31m    <<<<<<< HEAD\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## discovery cluster setup\n",
    "# CACHE_DIR = \"/scratch/lucchetti.f/models/\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = CACHE_DIR\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "from model_utils import *\n",
    "import tqdm as notebook_tqdm\n",
    "from trace_model import ModelLoader\n",
    "# from data.make import int_data, random_data, str_data\n",
    "torch.set_grad_enabled(False)\n",
    "from tqdm import tqdm \n",
    "from model_utils import layername\n",
    "#MODEL_NAME = \"Salesforce/codegen-16B-mono\"\n",
    "MODEL_NAME = \"bigcode/starcoder\"\n",
    "\n",
    "check_devs()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:27<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62136934400 / 84978434048 used for device 0, reserved 62138613760\n"
     ]
    }
   ],
   "source": [
    "lm = ModelLoader(MODEL_NAME)\n",
    "check_devs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for i < len(self.items):\\r\\n\\t\\titem = self.items[']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\"\"\"for i <\"\"\"]\n",
    "\n",
    "\n",
    "txt, ret_dict = lm.trace_generate(\n",
    "    prompts,\n",
    "    max_out_len=len(prompts[0]) + 10,\n",
    "    pick_greedily= False,\n",
    "    request_activations= [layername(lm.model, i) for i in range(27,30)], #lm.model.config.n_layer\n",
    "    request_logits= [layername(lm.model, i) for i in range(2,40)], #lm.model.config.n_layer\n",
    "    top_k = 10,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
