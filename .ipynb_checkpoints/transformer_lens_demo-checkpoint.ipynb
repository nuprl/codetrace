{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution of Attention Heads for Type recognition\n",
    "\n",
    "Trying out transformer lens with starcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 11 18:18:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA H100 PCIe    Off  | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    46W / 310W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA H100 PCIe    Off  | 00000000:65:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    44W / 310W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA H100 PCIe    Off  | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    47W / 310W |      5MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA H100 PCIe    Off  | 00000000:E3:00.0 Off |                    0 |\n",
      "| N/A   50C    P0    94W / 310W |  11891MiB / 81559MiB |     99%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    3   N/A  N/A   4011551      C   python                           2988MiB |\n",
      "|    3   N/A  N/A   4168079      C   .../portfolio-env/bin/python     1112MiB |\n",
      "|    3   N/A  N/A   4168080      C   .../portfolio-env/bin/python     1112MiB |\n",
      "|    3   N/A  N/A   4168081      C   .../portfolio-env/bin/python     1112MiB |\n",
      "|    3   N/A  N/A   4168082      C   .../portfolio-env/bin/python     1112MiB |\n",
      "|    3   N/A  N/A   4168083      C   .../portfolio-env/bin/python     1112MiB |\n",
      "|    3   N/A  N/A   4168084      C   .../portfolio-env/bin/python     1112MiB |\n",
      "|    3   N/A  N/A   4168085      C   .../portfolio-env/bin/python     1112MiB |\n",
      "|    3   N/A  N/A   4168086      C   .../portfolio-env/bin/python     1112MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franlucc/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "1\n",
      "0 / 84979089408 used for device 0, reserved 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franlucc/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:155: UserWarning:\n",
      "\n",
      "\n",
      "NVIDIA H100 PCIe with CUDA capability sm_90 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75 sm_80 sm_86.\n",
      "If you want to use the NVIDIA H100 PCIe GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from transformers import AutoConfig\n",
    "import gc\n",
    "import torch\n",
    "from model_utils import *\n",
    "import tqdm as notebook_tqdm\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "pio.renderers.default = \"notebook_connected\" # or use \"browser\" if you want plots to open with browser\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Optional, Callable, Tuple, Union\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from model_utils import to_hooked_config\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "#MODEL_NAME = \"Salesforce/codegen-16B-mono\"\n",
    "MODEL_NAME = \"bigcode/santacoder\"\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "!echo $CUDA_VISIBLE_DEVICES\n",
    "\n",
    "check_devs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Lens load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup from Transformer Lens tut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f826c4cfac0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## setup stuff\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", caxis=\"\", **kwargs):\n",
    "    return px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    return px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    return px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs)\n",
    "\n",
    "def plot_comp_scores(model: HookedTransformer, comp_scores: TT[\"heads\", \"heads\"], title: str = \"\", baseline: Optional[t.Tensor] = None) -> go.Figure:\n",
    "    return px.imshow(\n",
    "        utils.to_numpy(comp_scores),\n",
    "        y=[f\"L0H{h}\" for h in range(model.cfg.n_heads)],\n",
    "        x=[f\"L1H{h}\" for h in range(model.cfg.n_heads)],\n",
    "        labels={\"x\": \"Layer 1\", \"y\": \"Layer 0\"},\n",
    "        title=title,\n",
    "        color_continuous_scale=\"RdBu\" if baseline is not None else \"Blues\",\n",
    "        color_continuous_midpoint=baseline if baseline is not None else None,\n",
    "        zmin=None if baseline is not None else 0.0,\n",
    "    )\n",
    "\n",
    "import IPython\n",
    "from plotly.offline import init_notebook_mode\n",
    "    \n",
    "def enable_plotly_in_cell():\n",
    "    display(IPython.core.display.HTML('''<script src=\"/static/components/requirejs/require.js\"></script>'''))\n",
    "    init_notebook_mode(connected=False)\n",
    "\n",
    "t.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solutions_get_ablation_scores(model: HookedTransformer, tokens: TT[\"batch\", \"seq\"]) -> TT[\"n_layers\", \"n_heads\"]:\n",
    "    ablation_scores = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "    logits = model(tokens, return_type=\"logits\")\n",
    "    loss_no_ablation = cross_entropy_loss(logits, tokens)\n",
    "    for layer in tqdm(range(model.cfg.n_layers)):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            temp_hook_fn = functools.partial(head_ablation_hook, head_index_to_ablate=head)\n",
    "            patched_logits = model.run_with_hooks(tokens, fwd_hooks=[\n",
    "                (utils.get_act_name(\"result\", layer), temp_hook_fn)\n",
    "            ])\n",
    "            loss = cross_entropy_loss(patched_logits, tokens)\n",
    "            ablation_scores[layer, head] = loss - loss_no_ablation\n",
    "    return ablation_scores\n",
    "\n",
    "def solutions_mask_scores(attn_scores: TT[\"query_d_model\", \"key_d_model\"]):\n",
    "    mask = t.tril(t.ones_like(attn_scores)).bool()\n",
    "    neg_inf = t.tensor(-1.0e6).to(attn_scores.device)\n",
    "    masked_attn_scores = t.where(mask, attn_scores, neg_inf)\n",
    "    return masked_attn_scores\n",
    "\n",
    "def solutions_decompose_attn_scores(decomposed_q: t.Tensor, decomposed_k: t.Tensor) -> t.Tensor:\n",
    "    return einsum(\"q_comp q_pos d_model, k_comp k_pos d_model -> q_comp k_comp q_pos k_pos\", decomposed_q, decomposed_k)\n",
    "\n",
    "def solutions_find_K_comp_full_circuit(model: HookedTransformer, prev_token_head_index: int, ind_head_index: int) -> FactoredMatrix:\n",
    "    W_E = model.W_E\n",
    "    W_Q = model.W_Q[1, ind_head_index]\n",
    "    W_K = model.W_K[1, ind_head_index]\n",
    "    W_O = model.W_O[0, prev_token_head_index]\n",
    "    W_V = model.W_V[0, prev_token_head_index]\n",
    "    Q = W_E @ W_Q\n",
    "    K = W_E @ W_V @ W_O @ W_K\n",
    "    return FactoredMatrix(Q, K.T)\n",
    "\n",
    "def solutions_get_comp_score(\n",
    "    W_A: TT[\"in_A\", \"out_A\"], \n",
    "    W_B: TT[\"out_A\", \"out_B\"]\n",
    ") -> float:\n",
    "    W_A_norm = W_A.pow(2).sum().sqrt()\n",
    "    W_B_norm = W_B.pow(2).sum().sqrt()\n",
    "    W_AB_norm = (W_A @ W_B).pow(2).sum().sqrt()\n",
    "    return (W_AB_norm / (W_A_norm * W_B_norm)).item()\n",
    "\n",
    "def test_get_ablation_scores(ablation_scores: TT[\"layer\", \"head\"], model: HookedTransformer, rep_tokens: TT[\"batch\", \"seq\"]):\n",
    "    ablation_scores_expected = solutions_get_ablation_scores(model, rep_tokens)\n",
    "    t.testing.assert_close(ablation_scores, ablation_scores_expected)\n",
    "    print(\"All tests in `test_get_ablation_scores` passed!\")\n",
    "\n",
    "def test_full_OV_circuit(OV_circuit: FactoredMatrix, model: HookedTransformer, layer: int, head: int):\n",
    "        W_E = model.W_E\n",
    "        W_OV = FactoredMatrix(model.W_V[layer, head], model.W_O[layer, head])\n",
    "        W_U = model.W_U\n",
    "        OV_circuit_expected = W_E @ W_OV @ W_U\n",
    "        t.testing.assert_close(OV_circuit.get_corner(20), OV_circuit_expected.get_corner(20))\n",
    "        print(\"All tests in `test_full_OV_circuit` passed!\")\n",
    "\n",
    "def test_pos_by_pos_pattern(pattern: TT[\"n_ctx\", \"n_ctx\"], model: HookedTransformer, layer: int, head: int):\n",
    "    W_pos = model.W_pos\n",
    "    W_QK = model.W_Q[layer, head] @ model.W_K[layer, head].T\n",
    "    score_expected = W_pos @ W_QK @ W_pos.T\n",
    "    masked_scaled = solutions_mask_scores(score_expected / model.cfg.d_head ** 0.5)\n",
    "    pattern_expected = t.softmax(masked_scaled, dim=-1)\n",
    "    t.testing.assert_close(pattern[:50, :50], pattern_expected[:50, :50])\n",
    "    print(\"All tests in `test_full_OV_circuit` passed!\")\n",
    "\n",
    "def test_decompose_attn_scores(decompose_attn_scores: Callable, q: t.Tensor, k: t.Tensor):\n",
    "    decomposed_scores = decompose_attn_scores(q, k)\n",
    "    decomposed_scores_expected = solutions_decompose_attn_scores(q, k)\n",
    "    t.testing.assert_close(decomposed_scores, decomposed_scores_expected)\n",
    "    print(\"All tests in `test_decompose_attn_scores` passed!\")\n",
    "\n",
    "def test_find_K_comp_full_circuit(find_K_comp_full_circuit: Callable, model: HookedTransformer):\n",
    "    K_comp_full_circuit: FactoredMatrix = find_K_comp_full_circuit(model, 7, 4)\n",
    "    K_comp_full_circuit_expected: FactoredMatrix = solutions_find_K_comp_full_circuit(model, 7, 4)\n",
    "    assert isinstance(K_comp_full_circuit, FactoredMatrix), \"Should return a FactoredMatrix object!\"\n",
    "    t.testing.assert_close(K_comp_full_circuit.get_corner(20), K_comp_full_circuit_expected.get_corner(20))\n",
    "    print(\"All tests in `test_find_K_comp_full_circuit` passed!\")\n",
    "\n",
    "def test_get_comp_score(get_comp_score: Callable):\n",
    "    W_A = t.rand(3, 4)\n",
    "    W_B = t.rand(4, 5)\n",
    "    comp_score = get_comp_score(W_A, W_B)\n",
    "    comp_score_expected = solutions_get_comp_score(W_A, W_B)\n",
    "    assert isinstance(comp_score, float)\n",
    "    assert abs(comp_score - comp_score_expected) < 1e-5\n",
    "    print(\"All tests in `test_get_comp_score` passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2CustomConfig {\n",
      "  \"_name_or_path\": \"bigcode/santacoder\",\n",
      "  \"activation_function\": \"gelu_fast\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadCustomModel\"\n",
      "  ],\n",
      "  \"attention_head_type\": \"multiquery\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"bigcode/santacoder--configuration_gpt2_mq.GPT2CustomConfig\",\n",
      "    \"AutoModelForCausalLM\": \"bigcode/santacoder--modeling_gpt2_mq.GPT2LMHeadCustomModel\"\n",
      "  },\n",
      "  \"bos_token_id\": 49152,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 49152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_embd\": 2048,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": 8192,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49280\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "## cast config to Hooked config\n",
    "# print(vars(HookedTransformerConfig)['__annotations__'])\n",
    "cfg = to_hooked_config(model_config)\n",
    "print(model_config)\n",
    "\n",
    "# model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM: method inner handling of inputs, not sent to GPU device 0\n",
    " \n",
    "# from_pretrained_no_processing\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "show(lm.model, lm.model_type, lm.tokenizer, sentence_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
