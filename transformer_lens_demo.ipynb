{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution of Attention Heads for Type recognition\n",
    "\n",
    "Trying out transformer lens with starcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "0\r\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from transformers import AutoConfig\n",
    "import gc\n",
    "import torch\n",
    "import tqdm as notebook_tqdm\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "pio.renderers.default = \"notebook_connected\" # or use \"browser\" if you want plots to open with browser\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Optional, Callable, Tuple, Union\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from model_utils import bigcode_to_hooked_config, check_devs\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "#MODEL_NAME = \"Salesforce/codegen-16B-mono\"\n",
    "MODEL_NAME = \"bigcode/starcoder\"\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Lens load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup from Transformer Lens tut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f8b7c246920>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## setup stuff\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", caxis=\"\", **kwargs):\n",
    "    return px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    return px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    return px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs)\n",
    "\n",
    "def plot_comp_scores(model: HookedTransformer, comp_scores: TT[\"heads\", \"heads\"], title: str = \"\", baseline: Optional[t.Tensor] = None) -> go.Figure:\n",
    "    return px.imshow(\n",
    "        utils.to_numpy(comp_scores),\n",
    "        y=[f\"L0H{h}\" for h in range(model.cfg.n_heads)],\n",
    "        x=[f\"L1H{h}\" for h in range(model.cfg.n_heads)],\n",
    "        labels={\"x\": \"Layer 1\", \"y\": \"Layer 0\"},\n",
    "        title=title,\n",
    "        color_continuous_scale=\"RdBu\" if baseline is not None else \"Blues\",\n",
    "        color_continuous_midpoint=baseline if baseline is not None else None,\n",
    "        zmin=None if baseline is not None else 0.0,\n",
    "    )\n",
    "\n",
    "import IPython\n",
    "from plotly.offline import init_notebook_mode\n",
    "    \n",
    "def enable_plotly_in_cell():\n",
    "    display(IPython.core.display.HTML('''<script src=\"/static/components/requirejs/require.js\"></script>'''))\n",
    "    init_notebook_mode(connected=False)\n",
    "\n",
    "t.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solutions_get_ablation_scores(model: HookedTransformer, tokens: TT[\"batch\", \"seq\"]) -> TT[\"n_layers\", \"n_heads\"]:\n",
    "    ablation_scores = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "    logits = model(tokens, return_type=\"logits\")\n",
    "    loss_no_ablation = cross_entropy_loss(logits, tokens)\n",
    "    for layer in tqdm(range(model.cfg.n_layers)):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            temp_hook_fn = functools.partial(head_ablation_hook, head_index_to_ablate=head)\n",
    "            patched_logits = model.run_with_hooks(tokens, fwd_hooks=[\n",
    "                (utils.get_act_name(\"result\", layer), temp_hook_fn)\n",
    "            ])\n",
    "            loss = cross_entropy_loss(patched_logits, tokens)\n",
    "            ablation_scores[layer, head] = loss - loss_no_ablation\n",
    "    return ablation_scores\n",
    "\n",
    "def solutions_mask_scores(attn_scores: TT[\"query_d_model\", \"key_d_model\"]):\n",
    "    mask = t.tril(t.ones_like(attn_scores)).bool()\n",
    "    neg_inf = t.tensor(-1.0e6).to(attn_scores.device)\n",
    "    masked_attn_scores = t.where(mask, attn_scores, neg_inf)\n",
    "    return masked_attn_scores\n",
    "\n",
    "def solutions_decompose_attn_scores(decomposed_q: t.Tensor, decomposed_k: t.Tensor) -> t.Tensor:\n",
    "    return einsum(\"q_comp q_pos d_model, k_comp k_pos d_model -> q_comp k_comp q_pos k_pos\", decomposed_q, decomposed_k)\n",
    "\n",
    "def solutions_find_K_comp_full_circuit(model: HookedTransformer, prev_token_head_index: int, ind_head_index: int) -> FactoredMatrix:\n",
    "    W_E = model.W_E\n",
    "    W_Q = model.W_Q[1, ind_head_index]\n",
    "    W_K = model.W_K[1, ind_head_index]\n",
    "    W_O = model.W_O[0, prev_token_head_index]\n",
    "    W_V = model.W_V[0, prev_token_head_index]\n",
    "    Q = W_E @ W_Q\n",
    "    K = W_E @ W_V @ W_O @ W_K\n",
    "    return FactoredMatrix(Q, K.T)\n",
    "\n",
    "def solutions_get_comp_score(\n",
    "    W_A: TT[\"in_A\", \"out_A\"], \n",
    "    W_B: TT[\"out_A\", \"out_B\"]\n",
    ") -> float:\n",
    "    W_A_norm = W_A.pow(2).sum().sqrt()\n",
    "    W_B_norm = W_B.pow(2).sum().sqrt()\n",
    "    W_AB_norm = (W_A @ W_B).pow(2).sum().sqrt()\n",
    "    return (W_AB_norm / (W_A_norm * W_B_norm)).item()\n",
    "\n",
    "def test_get_ablation_scores(ablation_scores: TT[\"layer\", \"head\"], model: HookedTransformer, rep_tokens: TT[\"batch\", \"seq\"]):\n",
    "    ablation_scores_expected = solutions_get_ablation_scores(model, rep_tokens)\n",
    "    t.testing.assert_close(ablation_scores, ablation_scores_expected)\n",
    "    print(\"All tests in `test_get_ablation_scores` passed!\")\n",
    "\n",
    "def test_full_OV_circuit(OV_circuit: FactoredMatrix, model: HookedTransformer, layer: int, head: int):\n",
    "        W_E = model.W_E\n",
    "        W_OV = FactoredMatrix(model.W_V[layer, head], model.W_O[layer, head])\n",
    "        W_U = model.W_U\n",
    "        OV_circuit_expected = W_E @ W_OV @ W_U\n",
    "        t.testing.assert_close(OV_circuit.get_corner(20), OV_circuit_expected.get_corner(20))\n",
    "        print(\"All tests in `test_full_OV_circuit` passed!\")\n",
    "\n",
    "def test_pos_by_pos_pattern(pattern: TT[\"n_ctx\", \"n_ctx\"], model: HookedTransformer, layer: int, head: int):\n",
    "    W_pos = model.W_pos\n",
    "    W_QK = model.W_Q[layer, head] @ model.W_K[layer, head].T\n",
    "    score_expected = W_pos @ W_QK @ W_pos.T\n",
    "    masked_scaled = solutions_mask_scores(score_expected / model.cfg.d_head ** 0.5)\n",
    "    pattern_expected = t.softmax(masked_scaled, dim=-1)\n",
    "    t.testing.assert_close(pattern[:50, :50], pattern_expected[:50, :50])\n",
    "    print(\"All tests in `test_full_OV_circuit` passed!\")\n",
    "\n",
    "def test_decompose_attn_scores(decompose_attn_scores: Callable, q: t.Tensor, k: t.Tensor):\n",
    "    decomposed_scores = decompose_attn_scores(q, k)\n",
    "    decomposed_scores_expected = solutions_decompose_attn_scores(q, k)\n",
    "    t.testing.assert_close(decomposed_scores, decomposed_scores_expected)\n",
    "    print(\"All tests in `test_decompose_attn_scores` passed!\")\n",
    "\n",
    "def test_find_K_comp_full_circuit(find_K_comp_full_circuit: Callable, model: HookedTransformer):\n",
    "    K_comp_full_circuit: FactoredMatrix = find_K_comp_full_circuit(model, 7, 4)\n",
    "    K_comp_full_circuit_expected: FactoredMatrix = solutions_find_K_comp_full_circuit(model, 7, 4)\n",
    "    assert isinstance(K_comp_full_circuit, FactoredMatrix), \"Should return a FactoredMatrix object!\"\n",
    "    t.testing.assert_close(K_comp_full_circuit.get_corner(20), K_comp_full_circuit_expected.get_corner(20))\n",
    "    print(\"All tests in `test_find_K_comp_full_circuit` passed!\")\n",
    "\n",
    "def test_get_comp_score(get_comp_score: Callable):\n",
    "    W_A = t.rand(3, 4)\n",
    "    W_B = t.rand(4, 5)\n",
    "    comp_score = get_comp_score(W_A, W_B)\n",
    "    comp_score_expected = solutions_get_comp_score(W_A, W_B)\n",
    "    assert isinstance(comp_score, float)\n",
    "    assert abs(comp_score - comp_score_expected) < 1e-5\n",
    "    print(\"All tests in `test_get_comp_score` passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTBigCodeConfig {\n",
      "  \"_name_or_path\": \"bigcode/starcoder\",\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"GPTBigCodeForCausalLM\"\n",
      "  ],\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"inference_runner\": 0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_batch_size\": null,\n",
      "  \"max_sequence_length\": null,\n",
      "  \"model_type\": \"gpt_bigcode\",\n",
      "  \"multi_query\": true,\n",
      "  \"n_embd\": 6144,\n",
      "  \"n_head\": 48,\n",
      "  \"n_inner\": 24576,\n",
      "  \"n_layer\": 40,\n",
      "  \"n_positions\": 8192,\n",
      "  \"pad_key_length\": true,\n",
      "  \"pre_allocate_kv_cache\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attention_softmax_in_fp32\": true,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"validate_runner_input\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "hooked config: HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 48,\n",
      " 'd_mlp': 24576,\n",
      " 'd_model': 6144,\n",
      " 'd_vocab': -1,\n",
      " 'd_vocab_out': -1,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': True,\n",
      " 'initializer_range': 0.010206207261596576,\n",
      " 'model_name': 'bigcode/starcoder',\n",
      " 'n_ctx': 512,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 48,\n",
      " 'n_layers': 40,\n",
      " 'n_params': 14344519680,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': None,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'bigcode/starcoder',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "print(model_config)\n",
    "## cast config to Hooked config\n",
    "cfg = bigcode_to_hooked_config(model_config)\n",
    "print(\"hooked config:\", cfg)\n",
    "\n",
    "model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'g', 'pt', '2']\n",
      "tensor([[  0,  89, 385,  36]], device='cuda:0')\n",
      "<|endoftext|>gpt2\n"
     ]
    }
   ],
   "source": [
    "print(model.to_str_tokens(\"gpt2\"))             # --> ['<|endoftext|>', 'g', 'pt', '2']\n",
    "print(model.to_tokens(\"gpt2\"))                 # --> tensor([[50256, 70, 457, 17]], device='cuda:0')\n",
    "print(model.to_string([  0,  89, 385,  36]))   # --> '<|endoftext|>gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59828269056 / 84979089408 used for device 0, reserved 59829649408\n"
     ]
    }
   ],
   "source": [
    "check_devs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fibonacci(n):\n",
      "    \"\"\"\n",
      "    A function to calculate the nth fibonacci number\n",
      "    \"\"\"\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1 or n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "    \n",
      "def\n",
      "    Clazzpretation BI getterNemmeElementName Domrandidity\n",
      "Model loss: tensor(11.1859, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_description_text = '''def fibonacci(n):\n",
    "    \"\"\"\n",
    "    A function to calculate the nth fibonacci number\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    elif n == 1 or n == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "    \n",
    "def\n",
    "    '''\n",
    "\n",
    "print(model.generate(model_description_text))\n",
    "loss = model(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' }`', 'chn', 'Attrs', ' 파일', 'jwt', ' influence', '地', '(\"$', 'rus', 'allax', ' ENV', 'Refer', 'serverless', 'Bubble', ' insights', 'ást', 'Bubble', 'serverless', 'Tests', ' <>', '}!', '}!', 'rus', '所有的', ' insights', ' insights', 'allax', 'rus', ' collapse', 'allax', 'GetUser', 'imo', ' collapse', ' industry', ']}}', 'GetUser', 'GetUser', ' collapse', ' Quest', 'imo', 'ATFORM', ' insights', 'Presence', 'imo', ' collapse', ' insights', 'GetUser', ']}}', ' collapse', '以上', 'ATFORM', 'GetUser', 'imo', ' industry', '\"):', ' industry', ' insights', 'GetUser', '政府', 'GetUser', 'amil', ' insights', '媒', ' industry', 'Bre', ' nazw', '政府', ' insights'] \n",
      " tensor([  589, 28176, 34682,    26,    96,   711,   284,  1524,   284,   399,\n",
      "          667,   372,  9169,   322, 46245, 28176, 34682,  1451,   284,  1524,\n",
      "          284,   415,   310,   610,   225,    34,    44,   291,   442,   225,\n",
      "           34,   284,  4348,   310,   610,   225,    35,   556,   310,   610,\n",
      "          225,    36,    44,   291,   442,   225,    35,   284,   813,    44,\n",
      "          291,   442, 28176, 34682,    26,    96,    31,    35,    27,   474,\n",
      "        28176, 34682,    26,    96,    31,    36,    27, 15220],\n",
      "       device='cuda:0')\n",
      "Model accuracy: 0/68\n",
      "Correct words: []\n"
     ]
    }
   ],
   "source": [
    "logits = model(model_description_text, return_type=\"logits\")\n",
    "prediction = logits.argmax(dim=-1).squeeze()[:-1]\n",
    "true_tokens = model.to_tokens(model_description_text).squeeze()[1:]\n",
    "num_correct = (prediction == true_tokens).sum()\n",
    "\n",
    "print(model.to_str_tokens(prediction), '\\n', true_tokens)\n",
    "print(f\"Model accuracy: {num_correct}/{len(true_tokens)}\")\n",
    "print(f\"Correct words: {model.to_str_tokens(prediction[prediction == true_tokens])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
